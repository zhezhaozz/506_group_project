---
title: "Untitled"
output: html_document
---

```{r py, include=FALSE}
library(reticulate)
use_virtualenv("r-reticulate")
```
```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.formula.api as smf
from statsmodels.graphics.regressionplots import plot_partregress_grid
import matplotlib.axes as ax
import matplotlib.pyplot as plt

full_dt = pd.read_csv('cleaned_data_python.csv')
```

As we have got a nice cleaned dataset, we want to visualize our data with pair plots first. As shown in the pair plots for the systolic blood pressure, specifically from the scatter plots, we can find that there are weak relations or almost no relations between the variables. Moreover, from the histograms, we can find that the all the variables seem not to be normally distributed. Athough the variable systolic blood pressure and the variable bmi approach to be normally distributed, they are still right skewed. In addition, as shown in the pair plots for the diatolic blood pressure, from the scatter plots, again, we can find that there are weak relations or almost no relations between the variables. Also, from the histograms, except that the variable diatolic blood pressure seems to be almost normally distributed (only a little bit left skewed), all the other variables are not normally distributed.
```{python}
pair_plot_data_sys_bp = full_dt[['avg_BPXSY', 'gender', 'age', 'bmi', 'sleep', 
                                 'smoke', 'workhrs', 'avg_alcohol_freq_wk']]
pd.plotting.scatter_matrix(pair_plot_data_sys_bp, alpha = 0.5, figsize = (12, 12)); 
plt.xticks(fontsize = 2)
plt.yticks(fontsize = 2)
ax.Axes.set_xticklabels(pair_plot_data_sys_bp.columns, ha = 'center')
ax.Axes.set_yticklabels(pair_plot_data_sys_bp.columns, va = 'center');
plt.show()
```

```{python}
pair_plot_data_dia_bp = full_dt[['avg_BPXDI', 'gender', 'age', 'bmi', 'sleep', 
                                 'smoke', 'workhrs', 'avg_alcohol_freq_wk']]
pd.plotting.scatter_matrix(pair_plot_data_dia_bp, alpha = 0.5, figsize = (12, 12)) 
plt.xticks(fontsize = 2)
plt.yticks(fontsize = 2)
ax.Axes.set_xticklabels(pair_plot_data_dia_bp.columns, ha = 'center')
ax.Axes.set_yticklabels(pair_plot_data_dia_bp.columns, va = 'center');
plt.show()
```

After exploring the data through vizualizing with pair plots, we fit our linear regression models.

For the systolic blood pressure, we fit the model: 
$$
\begin{aligned}
 y &= \beta_{0} + \beta_{workhrs}*x_{workhrs} + \beta_{gender}*x_{gender} + \beta_{age}*x_{age} + \beta_{bmi}*x_{bmi} \notag\\
 &+ \beta_{sleep}*x_{sleep} + \beta_{smoke}*x_{smoke} + \beta_{avg\_alcohol\_freq\_wk}*x_{avg\_alcohol\_freq\_wk} + \epsilon
\end{aligned}
$$

And for diatolic blood pressure, we fit the model: 
$$
\begin{aligned}
 y &= \beta_{0} + \beta_{workhrs}*x_{workhrs} + \beta_{gender}*x_{gender} + \beta_{age}*x_{age} + \beta_{bmi}*x_{bmi} \notag\\
 &+ \beta_{sleep}*x_{sleep} +  \beta_{smoke}*x_{smoke} + \beta_{avg\_alcohol\_freq\_wk}*x_{avg\_alcohol\_freq\_wk} + \epsilon
\end{aligned}
$$

```{python systolic blood pressure}
model_formula_sys_bp = 'avg_BPXSY ~ gender + age + bmi + sleep + smoke + workhrs + avg_alcohol_freq_wk'
model_sys_bp = smf.ols(formula = model_formula_sys_bp, data = full_dt)
model_fit_sys_bp = model_sys_bp.fit()

print(model_fit_sys_bp.summary())

# fitted values (need a constant term for intercept)
model_fitted_y_sys_bp = model_fit_sys_bp.fittedvalues

# model residuals
model_resid_sys_bp = model_fit_sys_bp.resid

# standardized residuals
model_std_resid_sys_bp = model_fit_sys_bp.get_influence().resid_studentized_internal

# sqrt absolute standardized residuals
model_abs_sqrt_std_resid_sys_bp = np.sqrt(np.abs(model_std_resid_sys_bp))

# absolute residuals
model_abs_resid_sys_bp = np.abs(model_resid_sys_bp)
```
```{python diatolic blood pressure}
model_formula_dia_bp = 'avg_BPXDI ~ gender + age + bmi + sleep + smoke + workhrs + avg_alcohol_freq_wk'
model_dia_bp = smf.ols(formula = model_formula_dia_bp, data = full_dt)
model_fit_dia_bp = model_dia_bp.fit()

print(model_fit_dia_bp.summary())

# fitted values (need a constant term for intercept)
model_fitted_y_dia_bp = model_fit_dia_bp.fittedvalues

# model residuals
model_resid_dia_bp = model_fit_dia_bp.resid

# standardized residuals
model_std_resid_dia_bp = model_fit_dia_bp.get_influence().resid_studentized_internal

# sqrt absolute standardized residuals
model_abs_sqrt_std_resid_dia_bp = np.sqrt(np.abs(model_std_resid_dia_bp))

# absolute residuals
model_abs_resid_dia_bp = np.abs(model_resid_dia_bp)
```

From the summary results of the linear regression model for the systolic blood pressure, we can find that although the coefficient of the working hours is positive, which implies that working overtime increases the risk of high systolic blood pressure, this effect is not significant ($\beta = 1.58$, p = 0.557). As a result, we can conclude that working overtime cannot lead to abnormal systolic blood pressure. Besides, from the summary results of the linear regression model for the diatolic blood pressure, we can find that although the coefficient of the working hours is positive as well, which implies that working overtime increases the risk of high diatolic blood pressure too, this effect is not significant as well($\beta = 0.2246$, p = 0.905). As a result, we can conclude that working overtime also cannot lead to abnormal diatolic blood pressure. Therefore, in conclusion, working overtime also cannot lead to abnormal blood pressure.

After fitting the linear regression model, we want to check whether the following assumptions of the linear regression model are held:

1. Homoscedasticity: The variance of the residual is constant. 
2. Linearity: The relationship between between the independent and dependent variables is linear.
3. No or little multicollinearity: There is no or little collinearity between independent variables.

1. Homoscedasticity

In order to check the homoscedasticity assumption, we plotted residual plots for both systolic blood pressure and diatolic pressure.
```{python residual plot systolic blood pressure}
fig, ax = plt.subplots()
sns.residplot(model_fitted_y_sys_bp, model_resid_sys_bp, lowess = True,
              scatter_kws = {'alpha': 0.5}, 
              line_kws = {'color': 'red', 'lw': 1, 'alpha': 0.8})
plt.title('residual plot (systolic blood pressure)')
ax.set_xlabel('fitted values')
ax.set_ylabel('standardized residuals')
plt.show()
```
```{python residual plot diatolic blood pressure}
fig, ax = plt.subplots()
sns.residplot(model_fitted_y_dia_bp, model_resid_dia_bp, lowess = True,
              scatter_kws = {'alpha': 0.5}, 
              line_kws = {'color': 'red', 'lw': 1, 'alpha': 0.8})
plt.title('residual plot (diatolic blood pressure)')
ax.set_xlabel('fitted values')
ax.set_ylabel('standardized residuals')
plt.show()
```
From the residual plot of the systolic blood pressure, we can find the mean of the residual is almost 0, and the variance seems to be almost constant. Also, we have the same finds for the diatolic blood pressure. Therefore, the assumption of homoscedasticity can be considered as satisfied.

2. Linearity

In order to check the linearity assumption, we plotted partial regression plots for both systolic blood pressure and diatolic pressure against each of the independent variables.
```{python check linearity - added variable plots systolic blood pressure}
fig = plt.figure(figsize=(8, 12))
plot_partregress_grid(model_fit_sys_bp, fig=fig)
plt.show()
```

```{python check linearity - added variable plots diatolic blood pressure}
fig = plt.figure(figsize=(8, 12))
plot_partregress_grid(model_fit_dia_bp, fig=fig)
plt.show()
```
From the partial regression plots of the systolic blood pressure, we can find that for each of the independent variable, the expected value of the dependent variable (systolic blood pressure) is indeed a straight-line function of the independent variable, holding the others fixed. Also, from the partial regression plots of the diatolic blood pressurewe, we can get the same conclusion. Therefore, the assumption of linearity can be considered as satisfied.

3. No or little multicollinearity

In order to check the no/little-multicollinearity assumption, we computed the correlations between the continuous independent variables and the Pearson correlations between the binary independent variables.
```{python check correlations between the continuous variables}
continuous_vars = full_dt[['age', 'bmi', 'sleep', 'avg_alcohol_freq_wk']]
corr_continuous = np.corrcoef(continuous_vars.values.T)
# corr_continuous

fig, ax = plt.subplots(figsize=(12, 10))
sns.heatmap(corr_continuous, annot=True, square=True, annot_kws = {"size": 12})
plt.title("correlations between continuous variables", fontsize = 16)
ax.set_xticklabels(continuous_vars)
ax.set_yticklabels(continuous_vars, va = 'center')
ax.tick_params(axis='x', labelsize=12)
ax.tick_params(axis='y', labelsize=12)
plt.show()
```
```{python check correlations between categorical variables}
categorical_vars = full_dt[['gender', 'smoke', 'workhrs']]
categorical_vars.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson')
```
As shown in the heatmap of the correlations between the continuous independent variables, we can find that there are very small or almost no correlations between the continuous independent variables. Also, as the table of the Pearson correlations between the binary variables shows, we can find that there is little collinearity between each pair of the binary variables