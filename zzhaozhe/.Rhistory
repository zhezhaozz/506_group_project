cor_table$`p-value` = c(h, edu, g, dy, hv)
c(h, edu, g, dy, hv)
h
h = c()
edu = c()
g = c()
dy = c()
hv = c()
for(i in 1:5) {
h = append(h, cor.test(cor_df$hdi ,unlist(cor_df[,i]))$p.value )
edu = append(edu, cor.test(cor_df$l2educ ,unlist(cor_df[,i]))$p.value )
g = append(g, cor.test(cor_df$l2gni ,unlist(cor_df[,i]))$p.value )
dy = append(dy, cor.test(cor_df$dependency_young ,unlist(cor_df[,i]))$p.value )
hv = append(hv, cor.test(cor_df$high_violence ,unlist(cor_df[,i]))$p.value )
}
cor_table$`p-value` = c(h, edu, g, dy, hv)
cor_table
knitr::kable(cor_table, digits = 3) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T) %>%
collapse_rows(1)
knitr::kable(t, digits = 3) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T) %>%
collapse_rows(1)
knitr::kable(cor_table, digits = 3) %>%
kable_styling(full_width = F) %>%
column_spec(1, bold = T) %>%
collapse_rows(1)
install.packages('mvtnorm')
library(mvtnorm)
# part c
# if x is not normal
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = rmvt(n, mu = c(0,0,0,0,0), Sigma = sig)
# install.packages('mvtnorm')
library(mvtnorm)
x = rmvt(n, mu = c(0,0,0,0,0), Sigma = sig)
?rmvt
x = rmvt(n, Sigma = sig, df = 4)
x = rmvt(n, sigma = sig, df = 4)
beta = c( 6, -1, 3.5, 8, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
equa_list = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
equa_list[[i]] = ele
}
x = rmvt(n, sigma = sig, df = 4)
# part c
# if x is not normal
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = rmvt(n, sigma = sig, df = 4)
x
beta = c( 6, -1, 3.5, 8, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
f
y = f + e
equa_list = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
equa_list[[i]] = ele
}
# part c
# if x is not normal
set.seed(2019)
n = 1e6
p = 3
sig = 1 * diag(p)
x = rmvt(n, sigma = sig, df = 4)
beta = c( 6, 3.5, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
equa_list = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
equa_list[[i]] = ele
}
# part c
# if x is not normal
set.seed(2019)
n = 1e5
p = 3
sig = 1 * diag(p)
x = rmvt(n, sigma = sig, df = 4)
beta = c( 6, 3.5, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
equa_list = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
equa_list[[i]] = ele
}
E_y = Reduce("+", equa_y) / length(equa_y)
E_y = Reduce("+", equa_list) / length(equa_list)
norm(E_y - hessian, "F")
# part c
# if x is not normal
set.seed(2019)
n = 1e5
p = 3
sig = 1 * diag(p)
x = rmvt(n, sigma = sig, df = 30)
beta = c( 6, 3.5, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
equa_list = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
equa_list[[i]] = ele
}
E_y = Reduce("+", equa_list) / length(equa_list)
norm(E_y - hessian, "F")
library(MASS)
?eigen
n = 1e5
p = 5
# part d
set.seed(2019)
n = 1e5
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
beta = c( 6, 3.5, 7.6)
beta = c( 6, 3.5, 7.6, 1.2, 2)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
xx = x[i,] %*% t(x[i,])
xx = x[1,] %*% t(x[1,])
ele = (y[1] - mean(y) ) * xx
eigen(ele)
-9.023896e+01
1.111548e-14
1.111548e-4
# part d
set.seed(2019)
n = 1e5
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
beta = c( 6, 3.5, 7.6, 1.2, 2)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
beta_hat = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
beta_hat[[i]] = eigen(ele)$vector[,1]
}
sum( ( beta - beta_hat )^2 ) / n
beta_hat
beta
s = 0
for(j in 1:length(beta_hat)) {
s = s + sum( ( beta - beta_hat[[j]] )^2 )
}
s / n
# part d
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
beta = c( 6, 3.5, 7.6, 1.2, 2)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
# part d
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
beta = c( 6, 3.5, 7.6, 1.2, 2)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
beta_hat = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
beta_hat[[i]] = eigen(ele)$vector[,1]
}
s = 0
for(j in 1:length(beta_hat)) {
s = s + sum( ( beta - beta_hat[[j]] )^2 )
}
s / n
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache = TRUE)
# question 4 -------------------------------------------------------------------
# part b
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
library(MASS)
library(quantreg)
# install.packages('mvtnorm')
library(mvtnorm)
set.seed(2019)
# question 4 -------------------------------------------------------------------
# part b
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
beta = c( 6, -1, 3.5, 8, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
y = c()
xx_list = list()
for(i in 1:n) {
e = rnorm(1)
y[i] = f[i] + e
xx = x[i,] %*% t(x[i,])
xx_list[[i]] = xx
}
equa_y =  y - mean(y)
equa_mat = list()
for(j in 1:length(equa_y)){
ele = equa_y[j] * xx_list[[j]]
equa_mat[[j]] = ele
}
E_y = Reduce("+", equa_mat) / length(equa_mat)
norm(E_y - hessian, type = "F")
# part c
# if x is not normal
set.seed(2019)
n = 1e5
p = 3
sig = 1 * diag(p)
x = rmvt(n, sigma = sig, df = 30)
beta = c( 6, 3.5, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
e = rnorm(n)
y = f + e
equa_list = list()
for(i in 1:n) {
xx = x[i,] %*% t(x[i,])
ele = (y[i] - mean(y) ) * xx
equa_list[[i]] = ele
}
E_y_ng = Reduce("+", equa_list) / length(equa_list)
norm(E_y_ng - hessian, "F")
# part d
beta_hat = eigen(E_y)$vector[,1]
beta_hat
beta_hat / mean(beta_hat)
beta_hat / mean(2*beta_hat)
beta_hat/beta
eigen(E_y
)
beta = c( 6, -1, 3.5, 8, 7.6)
beta_hat/beta
beta_hat/mean(beta_hat)
beta_hat/2*mean(beta_hat)
beta_hat/mean(2*beta_hat)
2*beta_hat
beta_hat/mean(4*beta_hat)
beta_hat/mean(beta_hat/4)
beta_hat/4
# part d
beta_hat = eigen(E_y)$vector[,1]
beta_hat/beta
# part d
beta_hat = eigen(E_y)$vector[,1]
beta_hat/beta
# part d
beta_hat = eigen(E_y)$vector[,1]
constant = beta_hat/beta
constant
constant
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# question 4 -------------------------------------------------------------------
# part b
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
beta = c( 6, -1, 3.5, 8, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
y = c()
xx_list = list()
for(i in 1:n) {
e = rnorm(1)
y[i] = f[i] + e
xx = x[i,] %*% t(x[i,])
xx_list[[i]] = xx
}
equa_y =  y - mean(y)
equa_mat = list()
for(j in 1:length(equa_y)){
ele = equa_y[j] * xx_list[[j]]
equa_mat[[j]] = ele
}
E_y = Reduce("+", equa_mat) / length(equa_mat)
norm(E_y - hessian, type = "F")
# part d
beta_hat = eigen(E_y)$vector[,1]
constant = beta_hat/beta
constant
beta_hat
beta_hat/beta
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
# part d
beta_hat = eigen(E_y)$vector[,1]
# question 4 -------------------------------------------------------------------
# part b
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
library(MASS)
library(quantreg)
# install.packages('mvtnorm')
library(mvtnorm)
set.seed(2019)
# question 4 -------------------------------------------------------------------
# part b
set.seed(2019)
n = 1e6
p = 5
sig = 1 * diag(p)
x = mvrnorm(n, mu = c(0,0,0,0,0), Sigma = sig)
beta = c( 6, -1, 3.5, 8, 7.6)
f =  x %*% beta * x %*% beta
hessian = 2 *  ( beta %*% t(beta) )
y = c()
xx_list = list()
for(i in 1:n) {
e = rnorm(1)
y[i] = f[i] + e
xx = x[i,] %*% t(x[i,])
xx_list[[i]] = xx
}
equa_y =  y - mean(y)
equa_mat = list()
for(j in 1:length(equa_y)){
ele = equa_y[j] * xx_list[[j]]
equa_mat[[j]] = ele
}
E_y = Reduce("+", equa_mat) / length(equa_mat)
norm(E_y - hessian, type = "F")
# part d
beta_hat = eigen(E_y)$vector[,1]
constant = beta_hat/beta
constant
constant
# part d
beta_hat = eigen(E_y)$vector[,1]
constant = beta_hat/beta
constant
# part d
beta_hat = eigen(E_y)$vector[,1]
constant = beta_hat/beta
constant
library(data.table)
library(magrittr)
library(splines)
library(car)
# read the data
dt = fread("cleaned_data.csv")
# would difference between work overtimes but sleep for 8 hours and work overtimes
# while sleep for 5 hours less significant?
dt = dt[, wgt := 1]
d_fake_8hrs = dt[, `:=` (workhrs = 1, sleep = 8, wgt = 0)] # make fake data with contrast
d_fake_5hrs = dt[, `:=` (workhrs = 1, sleep = 5, wgt = 0)] # make fake data with contrast
View(dt)
# read the data
dt = fread("cleaned_data.csv")
# would difference between work overtimes but sleep for 8 hours and work overtimes
# while sleep for 5 hours less significant?
dt = dt[, wgt := 1]
d_fake_8hrs = dt %>%
.[, `:=` (workhrs = 1, sleep = 8, wgt = 0)] # make fake data with contrast
View(dt)
# read the data
dt = fread("cleaned_data.csv")
View(dt)
d_fake_8hrs = dt
d_fake_8hrs = d_fake_8hrs[, `:=` (workhrs = 1, sleep = 8, wgt = 0)] # make fake data with contrast
View(dt)
is.data.table(d_fake_8hrs)
View(dt)
# read the data
dt = fread("cleaned_data.csv")
d_fake_8hrs = dt[, `:=` (workhrs = 1, sleep = 8, wgt = 0)] # make fake data with contrast
# read the data
dt = fread("cleaned_data.csv")
View(d_fake_8hrs)
d_fake_5hrs = dt[, `:=` (workhrs = 1, sleep = 5, wgt = 0)] # make fake data with contrast
View(d_fake_5hrs)
View(d_fake_8hrs)
View(d_fake_5hrs)
View(d_fake_8hrs)
View(dt)
# read the data
dt = fread("cleaned_data.csv")
# would difference between work overtimes but sleep for 8 hours and work overtimes
# while sleep for 5 hours less significant?
dt = dt[, wgt := 1]
dx = rbindlist( list(dt, d_fake_8hrs, d_fake_5hrs) ) # combine tru data with fake data
result = lm( log(avg_sys_bp) ~ workhrs * (age + gender + bmi + bs(alchol, 5) +
bs(sleep, 5) + smoke) + smoke:alchol,
weights = wgt, data = dx) # fit regression with zero weights on fake data
pa = coef(result) # parameters
cm = vcov(result) # covariance matrix
dm = model.matrix(result) # design matrix
nx = 238
ct = dm[ (nx+1):(nx+2*nx), ]
ct = ct[1:nx,] - ct[(nx+1):(2*nx), ] # get the contrast
ct
znum = ct %*% pa # numerator of z-score
zdenom = sqrt(diag(ct %*% cm %*% t(ct))) # denominator of z-score
zscores = znum / zdenom
zscores
{mean(zscores) > z_criticle}
z_criticle = qnorm(0.975)
{mean(zscores) > z_criticle}
z_criticle
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(magrittr)
library(splines)
library(car)
library(effects)
library(dplyr)
# read the data
dt = fread("cleaned_data.csv")
analysis_dt = fread("cleaned_data.csv")
#read in data
df = read.csv("cleaned_data.csv", header = TRUE)
df = as_tibble(df)
df
#test the assumptions for the linear regression
#assumption1) normality assumption: check if the continuous variables are normal
#DVs
hist(df$SBP)
#read in data
df = read.csv("final_data.csv", header = TRUE)
df = as_tibble(df)
df
install.packages("reticulate")
library(reticulate)
use_python("/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7")
warnings()
library(reticulate)
use_python("/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7")
setwd("~/Desktop/Math_Courses/Umich/506/506_group_project/zzhaozhe")
library(reticulate)
use_python("/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7")
install.packages("reticulate")
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(magrittr)
library(splines)
library(car)
library(effects)
library(dplyr)
# read the data
dt = fread("cleaned_data.csv")
analysis_dt = fread("cleaned_data.csv")
library(reticulate)
use_python("/usr/local/bin/python3.7")
library(reticulate)
use_python("/usr/bin/python")
library(reticulate)
use_python("/usr/bin/python")
py_install("pandas")
library(reticulate)
use_python("/usr/bin/python")
py_install("pandas")
py_install("numpy")
py_install("seaborn")
py_install("statsmodels")
py_install("matplotlib")
install.packages("knitr")
install.packages("knitr")
install.packages("knitr")
install.packages("knitr")
